{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/albatrousse/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1370a16c-57c5-4276-aed8-68f7a22e7751"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 888"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une méthode généralement utilisée pour estimer les coefficients dans un modèle linéaire est la régression linéaire par moindre carré (OLS). Cette méthode a pour objectif de minimiser la somme des carrés des différences entre les valeurs observées et les valeurs prédites."
      ],
      "metadata": {
        "id": "S0qzHE7_UIFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Extraire les entrées et cibles des jeux de données\n",
        "X_train = train_set['inputs']\n",
        "t_train = train_set['targets']\n",
        "\n",
        "X_test = test_set['inputs']\n",
        "t_test = test_set['targets']\n",
        "\n",
        "# Créer et entraîner le modèle de régression linéaire\n",
        "linearReg = LinearRegression()\n",
        "linearReg.fit(X_train, t_train)\n",
        "\n",
        "# Afficher les coefficients et l'ordonnée à l'origine estimés\n",
        "print(\"Coefficients estimés (θ1, θ2, θ3) :\", linearReg.coef_)\n",
        "print(\"Ordonné à l'origine estimé (θ0) :\", linearReg.intercept_)\n",
        "\n",
        "# Évaluer le modèle sur les données de test\n",
        "score = linearReg.score(X_test, t_test)\n",
        "print(\"Score sur l'ensemble de test :\", score)\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dfaa53-2203-482d-e229-98f0c06c8b0e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients estimés (θ1, θ2, θ3) : [ 8.78948541  8.8947063  17.58357221]\n",
            "Ordonné à l'origine estimé (θ0) : 44.38097638349817\n",
            "Score sur l'ensemble de test : 0.9850486580679025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etant donné que le modèle à estimer est purement linéaire, une simple architecture linéaire permettrait de l'estimer. Au niveau de l'expressivité, une unique couche dense (avec une fonction d'activitation linéaire) a la même expressivité qu'un modèle de regréssion linéaire (cf représentation mathématique y = W*x + b). Cette \"simplicité\" permet également d'éviter le surapprentissage et donc favorise une meilleure généralisation.\n",
        "\n",
        "On choisit par conséquent, nous choisissons d'avoir une architecture avec une unique couche dense (sans activation) prenant en entrée trois predicteurs (x, y et z). La couche apprend les poids (θ1, θ2, θ3) et le biais (θ0) et trouve en sortie t."
      ],
      "metadata": {
        "id": "8rFpkOiXVx5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear = nn.Linear(in_features=3, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "model = SimpleNet()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "      # Remettre les gradients à zéro\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Propagation avant\n",
        "      outputs = model(batch_inputs)\n",
        "\n",
        "      # Calcul de la perte\n",
        "      loss = criterion(outputs, batch_targets.view(-1, 1))\n",
        "\n",
        "      # Rétropropagation\n",
        "      loss.backward()\n",
        "\n",
        "      # Mise à jour des paramètres\n",
        "      optimizer.step()\n",
        "\n",
        "    # Affichage de la perte toutes les 50 époques\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "      print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4731ba4-3261-4361-ae43-ae778f052ca7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 3400.1650\n",
            "Epoch [100/500], Loss: 3087.4531\n",
            "Epoch [150/500], Loss: 3111.8826\n",
            "Epoch [200/500], Loss: 3357.0037\n",
            "Epoch [250/500], Loss: 3281.1143\n",
            "Epoch [300/500], Loss: 3059.4094\n",
            "Epoch [350/500], Loss: 3215.1753\n",
            "Epoch [400/500], Loss: 3001.6572\n",
            "Epoch [450/500], Loss: 3326.0046\n",
            "Epoch [500/500], Loss: 3273.0959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des paramètres appris\n",
        "print(\"Coefficients estimés (θ1, θ2, θ3) :\", model.linear.weight.data.numpy())\n",
        "print(\"Ordonné à l'origne estimé (θ0) :\", model.linear.bias.data.numpy())"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d37c43-8c44-40c3-b786-cc32137b95ee"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients estimés (θ1, θ2, θ3) : [[-0.27973843  0.18755682 -0.04751011]]\n",
            "Ordonné à l'origne estimé (θ0) : [-0.2601475]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous nous attendions à avoir une perte sur le jeu de test similaire pour les deux méthodes mais étonnemment, la méthode avec un réseau de neuronne ne converge pas du tout. Cela pourrait être dû à une architecture trop simpliste, ou simplement qu'il faudrait plus de données pour le réseau afin d'estimer correctement les θ (Plutôt la première au vu des pertes lors de l'entrainement du réseau). Par manque de temps, je vais continuer vers la suite."
      ],
      "metadata": {
        "id": "-Bqu5QZgeO14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Prédictions sur le jeu de test\n",
        "predictionsSGD = model(torch.tensor(X_test, dtype=torch.float32))\n",
        "predictionsLinearReg = linearReg.predict(X_test)\n",
        "\n",
        "# Calcul de la perte sur le jeu de test\n",
        "lossSGD = criterion(predictionsSGD, torch.tensor(t_test, dtype=torch.float32).view(-1, 1))\n",
        "lossLinearReg = mean_squared_error(predictionsLinearReg, t_test)\n",
        "\n",
        "# Affichage de la perte sur le jeu de test\n",
        "print(\"Perte sur le jeu de test (SGD):\", lossSGD.item())\n",
        "print(\"Perte sur le jeu de test (LinearRegression):\", lossLinearReg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5-97wrccEz0",
        "outputId": "28854213-7bc6-46f7-f46d-2061d96e89e0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perte sur le jeu de test (SGD): 3147.55322265625\n",
            "Perte sur le jeu de test (LinearRegression): 3.66936534473806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd51fce-1f44-4d1a-f655-b3d7722a353c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "\n",
        "# Down 1 prend 64 canaux en entrée et 128 en sortie.\n",
        "# En regardant la structure d'un Down_causal dans util, on sait alors qu'il s'agit d'un MaxPool1D(2,2) suivit d'un Double_conv_causal\n",
        "# D'où le nombre de paramètre :\n",
        "# MaxPool1D : 0\n",
        "# Double_conv_causal in_ch = 64 et out_ch = 128 :\n",
        "  # conv1 : (3*64 + 1) * 128 = 24704\n",
        "  # conv2 : (3*128 + 1) * 128 = 49280\n",
        "  # bn1 et bn2 : chacun 2* 128 soit 512\n",
        "\n",
        "# TOTAL :  74 496\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Nombre total de paramètres: {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad86aadc-f4b1-4ecd-8a7a-4c01b9fff619"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres: 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?\n",
        "\n",
        " Le réseau réduit la taille d'entrée grâce à des opérations de mise en commun maximale avec des pas et des tailles de noyau variables. La deuxième partie du réseau restaure la taille d'origine en utilisant des techniques de suréchantillonnage dans les modules Up_causal, aidées par des skipconnexion et des paramètres soigneusement choisis pour les convolutions transposées. Ce processus permet au réseau d'extraire des caractéristiques à différentes échelles tout en conservant les informations spatiales pour la sortie finale."
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*.\n",
        "\n",
        "Le champ réceptif est augmenté par des convolutions en cascade et des convolutions dilatées. À la sortie de self.inc, la taille du champ récepteur est de 5, ce qui signifie que chaque caractéristique de sortie est influencée par un voisinage de 5 caractéristiques d'entrée."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69WMWCSZAg5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}